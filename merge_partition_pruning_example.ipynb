{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc299ed2-0f1d-457a-8f20-fbc5a2ed79c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, rand, when, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3d8de9-e8cd-4a79-85b9-3ae31628a468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_rows = 1000000000  \n",
    "\n",
    "# Generate initial DataFrame with partition column (values from 0 to 9)\n",
    "df = spark.range(0, num_rows) \\\n",
    "    .withColumn(\"partition_col\", (col(\"id\") % 10)) \\\n",
    "    .withColumn(\"name\", expr(\"concat('User_', id % 100000)\")) \\\n",
    "    .withColumn(\"department\", expr(\"CASE WHEN id % 3 = 0 THEN 'HR' WHEN id % 3 = 1 THEN 'IT' ELSE 'Finance' END\")) \\\n",
    "    .withColumn(\"salary\", (rand() * 10000).cast(\"int\"))\n",
    "\n",
    "(\n",
    "    df.repartition(500)\n",
    "      .write\n",
    "      .partitionBy(\"partition_col\")\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .saveAsTable(\"default.users\")\n",
    ")\n",
    "\n",
    "spark.sql('CREATE TABLE default.users_2 DEEP CLONE default.users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c68664f-39f0-4677-bd7b-2d00e32f2893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_df = spark.range(0, 10000000) \\\n",
    "    .withColumn(\"id\", (col(\"id\") * 100).cast(\"long\")) \\\n",
    "    .withColumn(\"partition_col\", when(col(\"id\") % 3 == 0, lit(1))\n",
    "                                  .when(col(\"id\") % 3 == 1, lit(3))\n",
    "                                  .otherwise(lit(7))) \\\n",
    "    .withColumn(\"name\", expr(\"concat('Updated_User_', id % 50000)\")) \\\n",
    "    .withColumn(\"department\", expr(\"CASE WHEN id % 4 = 0 THEN 'Sales' ELSE 'HR' END\")) \\\n",
    "    .withColumn(\"salary\", (rand() * 20000).cast(\"int\"))\n",
    "\n",
    "merge_df.createOrReplaceTempView('merge_source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f3cf6b-ad56-4227-8c98-d97bc5248c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MERGE de um source com 10.000.000 de linhas \n",
    "# Com um destination de 1.000.000.000 de linhas\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO default.users AS dest\n",
    "USING merge_source AS source\n",
    "ON dest.id = source.id \n",
    "  AND dest.partition_col = source.partition_col\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (id, partition_col, name, department, salary) \n",
    "      VALUES (source.id, source.partition_col, \n",
    "        source.name, source.department, source.salary\n",
    "    )\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95799f4e-bcf7-4b5d-a602-747cacd52968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate the list of partitions first\n",
    "partitions_str = ''\n",
    "for partition in spark.sql('SELECT DISTINCT partition_col FROM merge_source').collect():\n",
    "    partitions_str += str(partition['partition_col']) + ','\n",
    "\n",
    "partitions = partitions_str[:-1]\n",
    "\n",
    "# MERGE de um source com 10.000.000 de linhas \n",
    "# Com um destination de 1.000.000.000 de linhas\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO default.users_2 AS dest\n",
    "USING merge_source AS source\n",
    "ON dest.partition_col IN ({partitions})\n",
    "  AND dest.id = source.id \n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (id, partition_col, name, department, salary) \n",
    "      VALUES (source.id, source.partition_col, source.name, \n",
    "        source.department, source.salary\n",
    "      )\n",
    "\"\"\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8702435779999605,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "merge_partition_pruning_example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
